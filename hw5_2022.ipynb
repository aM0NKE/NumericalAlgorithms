{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535c17f1-f813-4c48-9bd8-dfc3856c9d07",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06d3e5bf55c941ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Homework set 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bf949-9252-4592-bde1-fab154d5018d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-736ff6bc3e0d0696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected (in the menubar, select Kernel â†’ Restart Kernel and Run All Cells...).\n",
    "\n",
    "Please **submit this Jupyter notebook through Canvas** no later than **Mon Dec. 5, 9:00**. **Submit the notebook file with your answers (as .ipynb file) and a pdf printout. The pdf version can be used by the teachers to provide feedback. A pdf version can be made using the save and export option in the Jupyter Lab file menu.**\n",
    "\n",
    "Homework is in **groups of two**, and you are expected to hand in original work. Work that is copied from another group will not be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f057a-3ad9-4b58-848b-690aab7d7de2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b13bc5ed16bce8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 0\n",
    "Write down the names + student ID of the people in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdaace4-9cfe-41cb-b26b-fee6914deec4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fd464f55ba436b1c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Christos Perchanidis    (14601206)\n",
    "\n",
    "River Vaudrin           (11877154)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd967a45-d51e-46ae-a464-402584619799",
   "metadata": {},
   "source": [
    "# The global keyword (helpful info for exercise 2)\n",
    "In exercise 2 you are asked, at some point, to count the number of times a certain function is evaluated. One way of doing this is using a global variable. To change a global variable x from inside a function, the global keyword is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a83140-bdd9-4e11-b53d-6719fe570a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x before: 4\n",
      "x after: 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to change a global variable x from inside a function, use the global keyword\n",
    "def foo():\n",
    "    global x\n",
    "    x = x*2\n",
    "    \n",
    "x=4\n",
    "print(\"x before:\", x) \n",
    "foo()\n",
    "print(\"x after:\", x)\n",
    "\n",
    "# verify for yourself that omitting the line \"global x\" produces an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776878a2-b6bd-4573-9dc0-cb3a9d68d553",
   "metadata": {},
   "source": [
    "-----\n",
    "# Exercise 1 (exercise 6.6(d), 2.5 pts)\n",
    "**N.B. This is a pen-and-paper exercise. If you prefer you may upload a separate pdf for this exercise and other pen-and-paper exercises. If you do, don't put both files in a single .zip file, upload them both separately.**\n",
    "\n",
    "Consider the minimization of \n",
    "$$\n",
    "  f(x,y) = x^2 + y^2\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "  g(x,y) = xy^2 -1 = 0 .\n",
    "$$\n",
    "Determine the critical points of the Lagrangian function for this problem and determine whether each is a constrained minimum, a constrained maximum, or neither."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d2058-1b1f-4c27-a5a5-4e1169cf6724",
   "metadata": {},
   "source": [
    "The gradient for $f$ is:\n",
    "$$\\nabla f(x,y) \n",
    "= \\left[\\begin{array}{c}\\frac{\\partial f(x,y)}{\\partial x} \\\\\\frac{\\partial f(x,y)}{\\partial y}\\end{array}\\right]\n",
    "= \\left[\\begin{array}{c}\\frac{\\partial}{\\partial x} x^2+y^2 \\\\\\frac{\\partial}{\\partial y}x^2+y^2\\end{array}\\right]\n",
    "= \\left[\\begin{array}{c} 2x \\\\ 2y \\end{array}\\right]$$\n",
    "\n",
    "The Jacobian matrix of $g(x,y)$ is:\n",
    "$$J_g(x, y)\n",
    "= \\left[\\begin{array}{c}\\frac{\\partial g(x,y)}{\\partial x} && \\frac{\\partial g(x,y)}{\\partial y}\\end{array}\\right]\n",
    "= \\left[\\begin{array}{c}\\frac{\\partial}{\\partial x} xy^2 -1 && \\frac{\\partial}{\\partial y}xy^2 -1\\end{array}\\right]\n",
    "=\\left[\\begin{array}{ll}y^2 && 2 x y\\end{array}\\right]$$\n",
    "\n",
    "The gradient of the *Lagrangian function* is:\n",
    "$$\\nabla \\mathcal{L}(x, y, \\lambda)\n",
    "=\\left[\\begin{array}{c}\n",
    "\\nabla_{x,y} \\mathcal{L}(x,y, \\lambda) \\\\\n",
    "\\nabla_\\lambda \\mathcal{L}(x, y, \\lambda)\n",
    "\\end{array}\\right]\n",
    "=\\left[\\begin{array}{c}\n",
    "\\nabla f(x, y)+J_g^T(x, y) \\lambda \\\\\n",
    "g(x,y)\n",
    "\\end{array}\\right]\n",
    "=\\left[\\begin{array}{c} 2 x+\\lambda y^2 \\\\ 2 y+2 \\lambda x y \\\\ x y^2-1\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "**TO BE COMPLETED:**\n",
    "$$B(x, y, \\lambda)\n",
    "=\\left[\\begin{array}{cc} 2 & 2 \\lambda y \\\\ 2 \\lambda y & 2+2 \\lambda x\n",
    "\\end{array}\\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd779b-f12d-433b-a7a1-48224d0a65b9",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce52e9-1099-4cf9-98b5-e322b7d7673b",
   "metadata": {},
   "source": [
    "## (a) (1 point)\n",
    "The Rosenbrock function is given by\n",
    "\n",
    "$$\n",
    "f(x,y) = 100 (y-x^2)^2 + (1-x)^2\n",
    "$$\n",
    "\n",
    "What is the gradient of $f$? Show that there is exactly one local minimum point and determine this point (N.B. this is a pen-and-paper exercise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87468d82-a8e3-42d8-9515-d531ca4acf72",
   "metadata": {},
   "source": [
    "The gradient for $f$ is:\n",
    "\n",
    "$$\\nabla f(x,y) \n",
    "= \\left[\\begin{array}{c}\\frac{\\partial f(x,y})}{\\partial x} \\\\\\frac{\\partial f(x,y})}{\\partial y}\\end{array}\\right]\n",
    "= \\left[\\begin{array}{c}\\frac{\\partial}{\\partial x} 100 (y-x^2)^2 + (1-x)^2 \\\\\\frac{\\partial}{\\partial y}100 (y-x^2)^2 + (1-x)^2\\end{array}\\right]\n",
    "=\\left[\\begin{array}{c} -400x(y-x^2)-2(1-x) \\\\ 200(y-x^2) \\end{array}\\right]$$\n",
    "\n",
    "The local minimum can be found by solving:\n",
    "\n",
    "$$\\nabla f(x,y) = \\left[\\begin{array}{c}0 \\\\ 0\\end{array}\\right]$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\left[\\begin{array}{c} -400x(y-x^2)-2(1-x) \\\\ 200(y-x^2) \\end{array}\\right] = \\left[\\begin{array}{c}0 \\\\ 0\\end{array}\\right]$$\n",
    "\n",
    "First we solve our first equation:\n",
    "\n",
    "$$-400x(y-x^2)-2(1-x)=0$$\n",
    "\n",
    "To obtain $y$:\n",
    "$$-400x(y-x^2)=2(1-x)$$\n",
    "$$\\frac{-400x(y-x^2)}{-400x}=\\frac{2(1-x)}{-400x}$$\n",
    "$$y-x^2=-\\frac{-x+1}{200x}$$\n",
    "$$y=-\\frac{-x+1}{200x}+x^2$$\n",
    "\n",
    "Then we substitute $y$ into our second equation:\n",
    "\n",
    "$$200(-\\frac{-x+1}{200x}+x^2-x^2)$$\n",
    "$$=-\\frac{-x+1}{x}$$\n",
    "\n",
    "And solve: \n",
    "\n",
    "$$-\\frac{-x+1}{x}=0$$\n",
    "\n",
    "To obtain:\n",
    "\n",
    "$$x=1$$\n",
    "\n",
    "Then we sustitute $x$ in our equation for $y$:\n",
    "\n",
    "$$y=-\\frac{-1+1}{200*1}+1^2$$\n",
    "\n",
    "Which gives us:\n",
    "\n",
    "$$y=1$$\n",
    "\n",
    "Thus our local minimum point is $(1,1)$.\n",
    "\n",
    "**NOTE: MAYBE ADD EXPLANATION ON WHY THIS IS THE ONLY MINIMUM?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b15a0-2bea-4861-a874-a0ac3fdba2f9",
   "metadata": {},
   "source": [
    "## (b) (2 points)\n",
    "Implement the method of steepest descent. Use `scipy.optimize.line_search` as line search method.\n",
    "\n",
    "Test your method on the Rosenbrock function starting from $(x,y) = (0,0)$.\n",
    "Plot the convergence to the minimum: Make a plot of the convergence in the $(x,y)$ plane as well as plot of the norm of the error as a function of the step number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39461b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Rosenbrock function: 100 * (y - x^2)^2 + (1 - x)^2\n",
    "    AND \n",
    "    Gradient of Rosenbrock function\n",
    "'''\n",
    "def rosenbrock(x):\n",
    "    return 100 * (x[0] - x[1]**2)**2 + (1 - x[0])**2\n",
    "def grad_rosen(x):\n",
    "    return np.array([-400*x[0]*(x[1]-x[0]**2)-2*(1-x[0]), 200*(x[1]-x[0]**2)])\n",
    "\n",
    "''' \n",
    "    Function from Heath example 6.13\n",
    "    AND\n",
    "    Gradient of that function\n",
    "'''\n",
    "def F(x):\n",
    "    return 0.5 * x[0]**2 + 2.5 * x[1]**2\n",
    "def grad_F(x):\n",
    "    return np.array([x[0], 5*x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b62ac",
   "metadata": {},
   "source": [
    "**NOTE: current results are for F(x) BUT should be for Rosenbrock(x)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b384d8b4-bf96-4b66-81b7-92dbd95c3099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Guess:\n",
      "x0=[[5]\n",
      " [1]]\n",
      "\n",
      "After 9 iteration our steepestDecent implmentation returned this solution for <function F at 0x11ed09d80>:\n",
      "x=[[ 0.13006147]\n",
      " [-0.02601229]]\n"
     ]
    }
   ],
   "source": [
    "def steepestDescent(x0, K, func=rosenbrock):\n",
    "    ''' \n",
    "        Steepest Descent method for unconstrained optimization\n",
    "    '''\n",
    "    # define functions\n",
    "    if func==F:\n",
    "        grad = grad_F\n",
    "    elif func==rosenbrock:\n",
    "        grad=grad_rosen\n",
    "\n",
    "    # initial guess\n",
    "    x = x0\n",
    "    for i, k in enumerate(range(K)):\n",
    "        # compute negative gradient\n",
    "        s = -grad(x) \n",
    "\n",
    "        # perform line search to find alpha\n",
    "        result = scipy.optimize.line_search(func, grad, x.reshape(1,2)[0], s.reshape(1,2)[0])\n",
    "        a = result[0]\n",
    "\n",
    "        # check if alpha is not none\n",
    "        if a == None:\n",
    "            print(\"No alpha could be found after {} iterations!\\n\".format(k))\n",
    "            break\n",
    "\n",
    "        # update solution\n",
    "        x = x + a*s\n",
    "\n",
    "    return x\n",
    "\n",
    "# Test data from book (WORKS!)\n",
    "func = F\n",
    "K = 9\n",
    "x0 = np.array([[5],[1]])\n",
    "\n",
    "# Assignment (NOT CONVERGING..)\n",
    "# func = rosenbrock\n",
    "# K = 10\n",
    "# x0 = np.zeros([2,1])\n",
    "\n",
    "print(\"Initial Guess:\\nx0={}\\n\".format(x0))\n",
    "print(\"After {} iteration our steepestDecent implmentation returned this solution for {}:\\nx={}\".format(K, func, steepestDescent(x0, K, func=func)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2ef08-a74e-4712-ad5f-acae905b5142",
   "metadata": {},
   "source": [
    "## (c) (1.5 points)\n",
    "\n",
    "Implement the BFGS method for unconstrained optimization, given in Heath chapter 6. Test the correctness of the code using the data in Example 6.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30895688-1c83-41b3-873d-17f37a906796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Guess:\n",
      "[[5]\n",
      " [1]]\n",
      "Initial Hessian:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "After 4 iteration our BFGS implementation returned this solution for F(x):\n",
      "x=[[-0.00915314]\n",
      " [-0.01533152]]\n",
      " AND \n",
      "B=[[0.99582089 0.03557777]\n",
      " [0.03557777 4.69711772]]\n",
      "\n",
      "These results are the same as the ones in the book, so we assume our implementation is correct!\n"
     ]
    }
   ],
   "source": [
    "def BFGS(x0, B0, K, func=F):\n",
    "    ''' \n",
    "        BFSG method for unconstrained optimization\n",
    "    '''\n",
    "    # define functions\n",
    "    if func==F:\n",
    "        grad = grad_F\n",
    "    elif func==rosenbrock:\n",
    "        grad=grad_rosen\n",
    "\n",
    "    # init list to store convergence behaviour\n",
    "    convergence = []\n",
    "\n",
    "    # initial guess\n",
    "    x = x0\n",
    "    # initial Hessian approx.\n",
    "    B = B0\n",
    "    # initial y\n",
    "    y = np.zeros([1,len(x)])\n",
    "    for k in range(K):\n",
    "        # compute gradient of x_k\n",
    "        Fx_k = grad(x)\n",
    "        # compute quasi-Newton step\n",
    "        s = np.linalg.solve(B, -Fx_k)\n",
    "\n",
    "        # update solution \n",
    "        x = x + s\n",
    "\n",
    "        # compute gradient of x_k+1\n",
    "        Fx_k1 = grad(x)\n",
    "        # compute y_k\n",
    "        y = Fx_k1 - Fx_k\n",
    "        \n",
    "        # stop if y=0 or s=0\n",
    "        if np.all(y==0) and np.all(s==0):\n",
    "            break\n",
    "        # update Hessian\n",
    "        B = B + (y@y.T)/(y.T@s) - (B@s@s.T@B)/(s.T@B@s)\n",
    "\n",
    "        # store current solution\n",
    "        convergence.append(x)\n",
    "    return x, B, convergence\n",
    "\n",
    "K = 4\n",
    "x0 = np.array([[5],[1]])\n",
    "B0 = np.identity(len(x0))\n",
    "x, B, convergence = BFGS(x0, B0, K, func=F)\n",
    "\n",
    "print(\"Initial Guess:\\n{}\".format(x0))\n",
    "print(\"Initial Hessian:\\n{}\\n\".format(B0))\n",
    "print(\"After {} iteration our BFGS implementation returned this solution for F(x):\\nx={}\\n AND \\nB={}\\n\".format(K, x, B))\n",
    "print(\"These results are the same as the ones in the book, so we assume our implementation is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efef4b-0200-4d10-97a1-f2c9a78d8579",
   "metadata": {},
   "source": [
    "## (d) (1 points)\n",
    "\n",
    "Apply your implementation of the BFGS method to find a local minimum of the Rosenbrock function (see previous exercise). Use starting point $(0,0)$ and do not assume any knowledge of the Hessian when you choose $B_0$.\n",
    "Plot the convergence to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ecf6a",
   "metadata": {},
   "source": [
    "**NOTE: Add plot for convergence to minimum (solution BFGS is correct for Rosenbrock!)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbb1e4b-cfe1-4aad-8b50-d7c01be3b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Guess:\n",
      "[[0.]\n",
      " [0.]]\n",
      "Initial Hessian:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "After 50 iteration our BFGS implementation came up with this solution rosenbrock(x): \n",
      "x=[[1.]\n",
      " [1.]]\n",
      " AND \n",
      "B=[[ 801.51464506 -399.78435443]\n",
      " [-399.78435443  199.90420735]]\n"
     ]
    }
   ],
   "source": [
    "K = 50\n",
    "x0 = np.zeros([2,1])\n",
    "B0 = np.identity(len(x0))\n",
    "x, B, convergence = BFGS(x0, B0, K, func=rosenbrock)\n",
    "\n",
    "print(\"Initial Guess:\\n{}\".format(x0))\n",
    "print(\"Initial Hessian:\\n{}\\n\".format(B0))\n",
    "print(\"After {} iteration our BFGS implementation came up with this solution rosenbrock(x): \\nx={}\\n AND \\nB={}\".format(K, x, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dfc5ce-542f-454c-924e-25de56a6fe7c",
   "metadata": {},
   "source": [
    "## (e) (1 point)\n",
    "How does the convergence compare to that of gradient descent (see\n",
    "previous question)? Let your program count the number of function and gradient evaluations and\n",
    "consider this in your comparison. Implement a stopping criterion in both methods that runs until $||x_k-x^*||_2 < 10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3b21f-2306-40e8-a659-05de85f9ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3e97b-9589-4bed-bf09-d7297719bee1",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
